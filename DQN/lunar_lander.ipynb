{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple sample funtion \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep q network definition  \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_fn(num_actions, e, state, policy_network):\n",
    "    \n",
    "    action_probabilities = torch.ones(num_actions).to(state.device) * (e/num_actions)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        highest_action_value = policy_network(state).max(1)[1].item()\n",
    "\n",
    "    action_probabilities[highest_action_value] += (1 - e)\n",
    "    \n",
    "    return action_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function represents the optimization steps. If our replay buffer is at capacity, we: \n",
    " -sample a batch of transitions from memory\n",
    " -Compute the expected value based on the policy network \n",
    " -Compute target values summing the observed reward with expected next state value using the target network \n",
    " -Compute loss between predicted values and target values and perform optimization on policy network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, policy_network, target_network, loss_fn, optimizer):\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    device = policy_network.device\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    \n",
    "    predicted_values = policy_network(states).gather(1, actions) #gathers all max value actions for actions in batch\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # compute bootstrapped target values\n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_episodes, batch_size, target_network, policy_network, gamma, epsilon, device, replay_buffer, optimizer, tau): \n",
    "    \n",
    "    for episode in range(num_episodes): \n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "        episode_end = True\n",
    "        \n",
    "        while not episode_end: \n",
    "            \n",
    "            action_probs = policy_fn(NUM_ACTIONS, epsilon, state, policy_network)\n",
    "            action = np.random.choice(np.arange(NUM_ACTIONS), p = action_probs[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_end = terminated or truncated \n",
    "            \n",
    "            if not terminated: \n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                    device = device).unsqueeze(0)\n",
    "            \n",
    "            replay_buffer.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            optimize_model(gamma, batch_size, replay_buffer, policy_network, target_network, optimizer)\n",
    "            \n",
    "            #extract network parameters for both networks \n",
    "            target_network_state_dict = target_network.state_dict()\n",
    "            policy_network_state_dict = policy_network.state_dict()\n",
    "            \n",
    "            for key in policy_network_state_dict.keys():\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_network_state_dict[key] = policy_network_state_dict[key]*tau + target_network_state_dict[key]*(1-tau)\n",
    "                target_network.load_state_dict(target_network_state_dict)\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
