{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       " 1.       ], (8,), float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observation is represented by an 8-dimensional box. Above, the first array indicates the lower bounds for different parameters whereas the second array specifies the upper bounds for those same parameters. Our 8 parameters in this case are: \n",
    "\n",
    "- Position X: Horizontal coordinate\n",
    "- Position Y: Vertical coordinate\n",
    "- Velocity X: Horizontal speed\n",
    "- Velocity Y: Vertical speed\n",
    "- Angle: The angle at which the lander is pointed\n",
    "- Angular Velocity: The speed at which the angle is changing\n",
    "- Left leg contact: Boolean value (1 if the left leg is in contact with the ground, 0 otherwise)\n",
    "- Right leg contact: Boolean value (1 if the right leg is in contact with the ground, 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a class for our replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple function to sample some experience from memory randomly \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, len_observation_space, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_fn(num_actions, e, state, policy_network):\n",
    "    \n",
    "    action_probabilities = torch.ones(num_actions).to(state.device) * (e/num_actions)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        highest_action_value = policy_network(state).max(1)[1].item()\n",
    "\n",
    "    action_probabilities[highest_action_value] += (1 - e)\n",
    "    \n",
    "    return action_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function Overview\n",
    "\n",
    "The following function represents the optimization steps. If our replay buffer is at capacity, we:\n",
    "\n",
    "- Sample a batch of transitions from memory. This initial step involves drawing a selection of varied transitions from our repository of stored experiences.\n",
    "\n",
    "- Compute the expected value using the policy network. This step requires us to determine probable outcomes and their associated probabilities, based on our current policy.\n",
    "\n",
    "- Compute target values by adding the observed reward to the expected next state value. This expected next state value is calculated using the target network instead of the policy network. \n",
    "\n",
    "- Compute the loss between the predicted and target values and then optimize the policy network. This final step is crucial for improving the predictive accuracy of our model. It works by learning from the discrepancies between predicted and actual outcomes, and uses these insights to refine the model.\n",
    "\n",
    "Through this sequence of steps, the function continuously enhances the model's predictive performance by learning from past errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, policy_network, target_network, loss_fn, optimizer):\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    device = policy_network.device\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions)) #batches our data as follows: ((state1, state2, ..), (action1, action2, ...), ...)\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    \n",
    "    predicted_values = policy_network(states).gather(1, actions) #gathers all max value actions for actions in batch\n",
    "    next_state_values = rewards.clone() # If the state is terminal, this is the q-value\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0] #we use our target network with our \n",
    "                                                                                            #sampled reward to give us a bootstrapped target\n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # if the state is non terminal, we add to it the expected value of the next state \n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_episodes, batch_size, target_network, policy_network, gamma, epsilon, device, replay_buffer, optimizer, tau): \n",
    "    \n",
    "    for episode in range(num_episodes): \n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "        episode_end = True\n",
    "        \n",
    "        while not episode_end: \n",
    "            \n",
    "            action_probs = policy_fn(NUM_ACTIONS, epsilon, state, policy_network)\n",
    "            action = np.random.choice(np.arange(NUM_ACTIONS), p = action_probs[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_end = terminated or truncated \n",
    "            \n",
    "            if not terminated: \n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                    device = device).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "            \n",
    "            replay_buffer.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            optimize_model(gamma, batch_size, replay_buffer, policy_network, target_network, optimizer)\n",
    "            \n",
    "            #extract network parameters for both networks \n",
    "            target_network_state_dict = target_network.state_dict()\n",
    "            policy_network_state_dict = policy_network.state_dict()\n",
    "            \n",
    "            for key in policy_network_state_dict.keys():\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_network_state_dict[key] = policy_network_state_dict[key]*tau + target_network_state_dict[key]*(1-tau)\n",
    "                target_network.load_state_dict(target_network_state_dict)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams \n",
    "NUM_EPISODES = 600\n",
    "E = 0.1\n",
    "GAMMA = 1\n",
    "ALPHA = 0.1\n",
    "BATCH_SIZE = 8 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayMemory(8)\n",
    "policy_network = DQN(n_actions=NUM_ACTIONS, len_observation_space=8)\n",
    "target_network = DQN(n_actions=NUM_ACTIONS, len_observation_space=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
