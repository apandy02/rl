{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla (average based) On-Policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: OpenAI Taxi-v3\n",
    "\n",
    "For the purposes of this assignment, we use Taxi-v3 from OpenAI's Gym repository. This environment, its states, actions and goals are detailed on the following web page: https://www.gymlibrary.dev/environments/toy_text/taxi/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have Discrete(6) action space and Discrete(500) state space\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "print(\"We have {} action space and {} state space\".format(action_space, state_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-policy Monte Carlo Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns an array of action probabilities for a given state (a polic) \n",
    "#this policy is designed to be epsilon-greedy in relation to the state action value function Q \n",
    "def policy_fn(Q, num_actions, e, state):\n",
    "    action_probabilities = np.ones(num_actions) * (e/num_actions)\n",
    "    highest_action_value = np.argmax(Q[state])\n",
    "    action_probabilities[highest_action_value] += 1 - e\n",
    "    \n",
    "    return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "e = 0.5\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 2\n",
      "Episode: 0 Epoch: 3\n",
      "Episode: 0 Epoch: 4\n",
      "Episode: 0 Epoch: 5\n",
      "Episode: 0 Epoch: 6\n",
      "Episode: 0 Epoch: 7\n",
      "Episode: 0 Epoch: 8\n",
      "Episode: 0 Epoch: 9\n",
      "Episode: 0 Epoch: 10\n",
      "Episode: 0 Epoch: 11\n"
     ]
    }
   ],
   "source": [
    "Q = np.ones((state_space.n, action_space.n))\n",
    "returns = [[[]]*action_space.n for i in range(state_space.n)]\n",
    "pi = np.zeros((state_space.n, action_space.n))\n",
    "\n",
    "epochs_per_episode = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0 \n",
    "    epoch = 0\n",
    "    terminated = False \n",
    "    while not terminated: \n",
    "        \n",
    "        epoch+=1\n",
    "        print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "        if type(state)==tuple: \n",
    "            state = state[0]\n",
    "        \n",
    "        pi[state] = policy_fn(Q, action_space.n, e, state)\n",
    "        action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "        next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        #print(episode[(state, action)])\n",
    "        if terminated: \n",
    "            epochs_per_episode.append(epoch)\n",
    "            break \n",
    "        state = next_state\n",
    "        \n",
    "    if i % 10: \n",
    "        e-=0.1    \n",
    "    visited = []\n",
    "    return_till_now = 0 \n",
    "    returns = {}\n",
    "    states_in_episode = []\n",
    "    for k, (state, action, reward) in enumerate(episode): \n",
    "        if state not in states_in_episode: \n",
    "            states_in_episode.append(state)\n",
    "        if (state, action) in visited: \n",
    "            continue\n",
    "        else: \n",
    "            visited.append((state, action))\n",
    "            G = sum([r*(gamma**j) for j, (s, a, r) in enumerate(episode)])\n",
    "            if (state, action) in returns.keys(): \n",
    "                returns[(state, action)].append(G)\n",
    "            else: \n",
    "                returns[(state, action)] = [G]\n",
    "            Q[state][action] = sum(returns[(state, action)])/len(returns[(state, action)])\n",
    "            \n",
    "    for state in states_in_episode: \n",
    "        pi[state] = policy_fn(Q, action_space.n, e, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.15, 0.15, 0.25, 0.15, 0.15, 0.15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       ...,\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [-2.324e+03, -2.324e+03,  1.000e+00, -2.324e+03,  1.000e+00,\n",
       "        -2.324e+03]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_start = env.reset()\n",
    "i = 0\n",
    "e = 0.03 #make policy less exploratory while testing \n",
    "for i in range(1000): \n",
    "    if type(state)==tuple: \n",
    "        state = state[0]\n",
    "    \n",
    "    env.render()\n",
    "    #time.sleep(0.03)\n",
    "    pi[state] = policy_fn(Q, action_space.n, e, state)\n",
    "    action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "    next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "    print(\"At step {}, reward = {} and termination: {}\".format(i, reward, terminated))\n",
    "    \n",
    "    #print(episode[(state, action)])\n",
    "    if terminated: \n",
    "        break \n",
    "    state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "Due to the extremely stochastic nature of the average based monte carlo algorithm during the first few episodes, runtime per episode was too high to keep running. However, this led to some learnings: As the size of the state and action spaces grows, the number of epochs required to reach a terminal state grows rapidly. Since this method learns offline (ie. not updating its values during interaction) the learning through the first few episodes is extremely slow. In the next notebook I will implement a temporal difference learning model that learns online and compare these two results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
