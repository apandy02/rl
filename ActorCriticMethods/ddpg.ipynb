{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetah-v4', render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (6,), float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_ACTIONS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (17,), float64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_OBSERVATIONS = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a class for our replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple function to sample some experience from memory randomly \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Critic (Q-Value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_Q(nn.Module):\n",
    "\n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(critic_Q, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space + dim_actions, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        x = torch.cat([observation, action], dim=-1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_Policy(nn.Module): \n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(actor_Policy, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #deterministically outputting a single action \n",
    "        self.layer3 = nn.Linear(128, dim_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.tanh(self.layer3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function Overview\n",
    "\n",
    "The following function represents the optimization steps. If our replay buffer is at capacity, we:\n",
    "\n",
    "- Sample a batch of transitions from memory. This initial step involves drawing a selection of varied transitions from our repository of stored experiences.\n",
    "\n",
    "- Compute the expected value using the policy network. This step requires us to determine probable outcomes and their associated probabilities, based on our current policy.\n",
    "\n",
    "- Compute target values by adding the observed reward to the expected next state value. This expected next state value is calculated using the target network instead of the policy network. \n",
    "\n",
    "- Compute the loss between the predicted and target values and then optimize the policy network. This final step is crucial for improving the predictive accuracy of our model. It works by learning from the discrepancies between predicted and actual outcomes, and uses these insights to refine the model.\n",
    "\n",
    "Through this sequence of steps, the function continuously enhances the model's predictive performance by learning from past errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, actor, critic, target_actor, target_critic, loss_fn, actor_optimizer, critic_optimizer, device):\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions)) #batches our data as follows: ((state1, state2, ..), (action1, action2, ...), ...)\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    \n",
    "    actions = actions.unsqueeze(-1)\n",
    "    predicted_values = critic(states, actions) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = target_actor(non_final_next_states)  \n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_critic(non_final_next_states, next_actions)  \n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # if the state is non terminal, we add to it the expected value of the next state \n",
    "    td_error = target_values - predicted_values \n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(critic.parameters(), 100)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    log_action = torch.log(actions)\n",
    "    actor_loss = - log_action * td_error\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_episodes, batch_size, target_actor, actor, target_critic, critic, gamma, epsilon, device, replay_buffer, actor_optimizer, critic_optimizer, tau): \n",
    "    \n",
    "    reward_per_episode = []\n",
    "\n",
    "    for episode in range(num_episodes): \n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "        episode_end = False\n",
    "        step = 0 \n",
    "        cum_reward = 0\n",
    "\n",
    "        if  not episode % 100 and episode < 400: \n",
    "            epsilon-=0.1\n",
    "\n",
    "        while not episode_end:\n",
    "            action = actor(state).squeeze().cpu().detach().numpy()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            step+=1\n",
    "            cum_reward+=reward\n",
    "            \n",
    "            episode_end = terminated or truncated \n",
    "            \n",
    "            if not terminated: \n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                    device = device).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "            else: \n",
    "                next_state = None\n",
    "            \n",
    "            action = torch.tensor(action, dtype=torch.int64, device=device).unsqueeze(0)\n",
    "            reward = torch.tensor(reward, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            replay_buffer.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn=torch.nn.MSELoss(size_average=True), actor_optimizer=actor_optimizer,\\\n",
    "                           critic_optimizer= critic_optimizer, device=device)\n",
    "            \n",
    "            if not episode % (num_episodes / 50) and episode > 50:\n",
    "                #extract network parameters for both networks \n",
    "                target_critic_state_dict = target_critic.state_dict()\n",
    "                policy_network_state_dict = critic.state_dict()\n",
    "\n",
    "                target_actor_state_dict = target_actor.state_dict()\n",
    "                policy_network_state_dict = actor.state_dict()\n",
    "                \n",
    "                for key in policy_network_state_dict.keys():\n",
    "                    # θ′ ← τ θ + (1 −τ )θ′\n",
    "                    target_critic_state_dict[key] = policy_network_state_dict[key]*tau + target_critic_state_dict[key]*(1-tau)\n",
    "                    target_critic.load_state_dict(target_critic_state_dict)\n",
    "\n",
    "                    target_actor_state_dict[key] = policy_network_state_dict[key]*tau + target_actor_state_dict[key]*(1-tau)\n",
    "                    target_actor.load_state_dict(target_actor_state_dict)\n",
    "            \n",
    "        print(\"Episode: {} Score: {}\".format(episode, cum_reward))\n",
    "        reward_per_episode.append(cum_reward)\n",
    "        clear_output(wait=True) # Clear the output of the current cell receiving output\n",
    "        plt.plot(reward_per_episode)\n",
    "        plt.title('Cumulative Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams \n",
    "NUM_EPISODES = 500 # this is abitrary \n",
    "E = 0.5 #we start off with a relatively high exploration rate and then decay that over training steps to start exploiting more \n",
    "GAMMA = 1 #\n",
    "ALPHA = 3e-4\n",
    "BATCH_SIZE = 32 #we want a relatively large batch size to leverage GPU parallelism but at the same time we want this number to be far \n",
    "                #smaller than our memory capacity so that we can sample randomly at the same time \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = ReplayMemory(100000)\n",
    "\n",
    "actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "actor.to(DEVICE)\n",
    "target_actor.to(DEVICE)\n",
    "\n",
    "critic.to(DEVICE)\n",
    "target_critic.to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer(num_episodes\u001b[39m=\u001b[39;49mNUM_EPISODES, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, actor\u001b[39m=\u001b[39;49mactor, critic\u001b[39m=\u001b[39;49mcritic, target_actor\u001b[39m=\u001b[39;49mtarget_actor, target_critic\u001b[39m=\u001b[39;49mtarget_critic,\\\n\u001b[1;32m      2\u001b[0m          gamma\u001b[39m=\u001b[39;49mGAMMA, epsilon\u001b[39m=\u001b[39;49mE, device\u001b[39m=\u001b[39;49mDEVICE, replay_buffer\u001b[39m=\u001b[39;49mreplay_buffer, actor_optimizer\u001b[39m=\u001b[39;49mactor_optimizer, critic_optimizer\u001b[39m=\u001b[39;49mcritic_optimizer, tau\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[108], line 37\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(num_episodes, batch_size, target_actor, actor, target_critic, critic, gamma, epsilon, device, replay_buffer, actor_optimizer, critic_optimizer, tau)\u001b[0m\n\u001b[1;32m     34\u001b[0m replay_buffer\u001b[39m.\u001b[39mpush(state, action, next_state, reward)\n\u001b[1;32m     35\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m---> 37\u001b[0m optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mMSELoss(size_average\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), actor_optimizer\u001b[39m=\u001b[39;49mactor_optimizer,\\\n\u001b[1;32m     38\u001b[0m                critic_optimizer\u001b[39m=\u001b[39;49m critic_optimizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m episode \u001b[39m%\u001b[39m (num_episodes \u001b[39m/\u001b[39m \u001b[39m50\u001b[39m) \u001b[39mand\u001b[39;00m episode \u001b[39m>\u001b[39m \u001b[39m50\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[39m#extract network parameters for both networks \u001b[39;00m\n\u001b[1;32m     42\u001b[0m     target_critic_state_dict \u001b[39m=\u001b[39m target_critic\u001b[39m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[107], line 18\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(gamma, batch_size, memory, actor, critic, target_actor, target_critic, loss_fn, actor_optimizer, critic_optimizer, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(batch\u001b[39m.\u001b[39mreward)\n\u001b[1;32m     17\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m predicted_values \u001b[39m=\u001b[39m critic(states, actions) \n\u001b[1;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m     next_actions \u001b[39m=\u001b[39m target_actor(non_final_next_states)  \n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[105], line 13\u001b[0m, in \u001b[0;36mcritic_Q.forward\u001b[0;34m(self, observation, action)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation, action):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([observation, action], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x))\n\u001b[1;32m     15\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer(num_episodes=NUM_EPISODES, batch_size=BATCH_SIZE, actor=actor, critic=critic, target_actor=target_actor, target_critic=target_critic,\\\n",
    "         gamma=GAMMA, epsilon=E, device=DEVICE, replay_buffer=replay_buffer, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, tau=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
