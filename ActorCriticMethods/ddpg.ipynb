{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "In this notebook we explore a neural network based implementation of the Actor-Critic method for Reinforcement Learning. This algorithm can be seen as the convergence of the breakthroughs that came from the DQN paper with the actor-critic algorithm. This algorithm, DDPG, applies the concepts of experience replay using a replay buffer to mitigate the sequential nature of the data being sampled by interaction with the environment, and instead learns by sampling a batch of random samples of experience saved in the replay buffer for gradient based optimization. \n",
    "\n",
    "In general, actor-critic algorithms comprise of two parts, an actor and a critic. Both of these parts are learned from data in the form of multilayer perceptrons rather than manual mappings or hand engineered features. The actor here is our policy network. Given an observation as input, the actor picks out the best action possible based on its weights. Since our actor directly picks out a singular action instead of a probability of actions, we say this follows the \"deterministic\" policy gradients algorithm, thus the \"deterministic\" in DDPG. The critic, on the other hand, is used to provide feedback to the actor. The critic network predicts the expected value of a state action pair (action picked out by the actor network), which is what is our actor's goal to maximize.\n",
    "\n",
    "We use the concept of a target network (as in DQN, refer to /reinforcement_learning/DQN for more) in order to stabilize the learning process. \n",
    "\n",
    "DDPG was designed to work well in continuous action spaces, so we run our experiences on the mujoco Half-Cheetah-v4 environment that provides us with a \"half\" cheetah as an agent whose goal is to travel as much in the forward direction as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00763375,  0.03125413, -0.00071709, -0.03314128,  0.02287299,\n",
       "        -0.06734332, -0.04080408,  0.02668012, -0.24806516,  0.16761543,\n",
       "         0.00586194,  0.12995108,  0.04826599,  0.01424354,  0.05099084,\n",
       "        -0.02589471, -0.02283937]),\n",
       " {})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v4')#, render_mode = \"human\") uncomment to render \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "As mentioned previously, the DDPG algorithm shines in continuous action spaces. Our cheetah has 6 pivots upon which it can execute forces from the -1.0 to 1.0 range, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (6,), float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_ACTIONS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (17,), float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 17 dimensional observation space. Our observations consist of positional values of the different pivots on the cheetah, their velocities and angular velocities with respect to our different euclidean coordinate frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_OBSERVATIONS = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a class for our replay buffer\n",
    "Nothing changes here v/s the DQN implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple function to sample some experience from memory randomly \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Critic (Q-Value function)\n",
    "The differences between this network and the Deep Q Network used for pure DQN are: \n",
    "- We're now evaluating a single action that has already been selected by the actor so the first linear layer accepts our observation concatenated with our action \n",
    "- Since we're evaluating only one action, we output the value estimate for that singluar action only instead of the set of possible actions giving our network an output dimension of 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_Q(nn.Module):\n",
    "\n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(critic_Q, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space + dim_actions, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        x = torch.cat([observation, action], dim=-1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network \n",
    "A multilayer perceptron that accepts our observation as input and outputs a deterministic action to be taken at timestep t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_Policy(nn.Module): \n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(actor_Policy, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #deterministically outputting a single action \n",
    "        self.layer3 = nn.Linear(128, dim_actions) #output has the specified dimensionality to specify actions with respect to \n",
    "                                                  #all action dimensions (in the case of our example, this is 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.tanh(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AFTER_STEPS = 5e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function Overview\n",
    "\n",
    "The following function represents the optimization steps. If our replay buffer is at capacity, we:\n",
    "\n",
    "- Sample a batch of transitions from memory. This initial step involves drawing a selection of varied transitions from our repository of stored experiences.\n",
    "\n",
    "- Compute the expected value using the critic network. \n",
    "\n",
    "- Run inference on target network to determine what our \"next action\" would be given our sampled \"next_state\"\n",
    "\n",
    "- Compute the target value by running inference on the target critic given the next state and next action from the above step \n",
    "\n",
    "- Given the predicted value, sampled reward and target next state/action value, compute the TD error\n",
    "\n",
    "- Compute the loss between the predicted and target values and then optimize the critic network. \n",
    "\n",
    "- Use the negative log of the mean value of the values outputted by the critic as the loss for the actor. We want to maximize this mean. \n",
    "\n",
    "Through this sequence of steps, the function continuously enhances the model's predictive performance along with control performance by learning from past errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, actor, critic, target_actor, target_critic, loss_fn, actor_optimizer, critic_optimizer, device):\n",
    "    \n",
    "    if len(memory) < 25e3: \n",
    "        print(\"not enough samples\")\n",
    "        return None, None\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions)) #batches our data as follows: ((state1, state2, ..), (action1, action2, ...), ...)\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state).to(device)\n",
    "    actions = torch.tensor(np.stack(batch.action), dtype=torch.float32, device=device)\n",
    "    rewards = torch.tensor(np.stack(batch.reward), dtype=torch.float32, device=device)\n",
    "\n",
    "    \n",
    "    predicted_values = critic(states, actions) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = target_actor(non_final_next_states)  \n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_critic(non_final_next_states, next_actions).squeeze(1)  \n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # if the state is non terminal, we add to it the expected value of the next state \n",
    "    td_error = target_values - predicted_values \n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(critic.parameters(), 100)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    actor_loss = -critic(states,actor(states)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    return loss.cpu().detach().numpy(), actor_loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_epochs, num_cycles, num_rollouts, train_steps, batch_size, target_actor, actor, target_critic, critic, gamma, device, \\\n",
    "            replay_buffer, actor_optimizer, critic_optimizer, tau, epochs_till_buffer_full): \n",
    "    \n",
    "    reward_per_episode = []\n",
    "    actor_loss_per_episode = []\n",
    "    critic_loss_per_episode = []\n",
    "    noise_factor = 0.3 \n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "\n",
    "        rewards_per_cycle = []\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        \n",
    "        for cycle in range(num_cycles): \n",
    "\n",
    "            cum_reward = 0\n",
    "            epoch_end = False\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "            \n",
    "            for rollout in range(num_rollouts):\n",
    "\n",
    "                #select action based on our policy network \n",
    "                action = actor(state).squeeze().cpu().detach().numpy()\n",
    "                \n",
    "                #Add noise to our action vector to introduce some level of exploration \n",
    "                noise = np.random.normal(0, noise_factor, size=env.action_space.shape) # Adjust the scale (0.2) as needed\n",
    "                \n",
    "                action = np.clip(action + noise, env.action_space.low, env.action_space.high) #clip so that our actions never exceed the allowed thresholds \n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                epoch_end = terminated or truncated\n",
    "\n",
    "                cum_reward+=reward\n",
    "                \n",
    "                if not terminated: \n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                        device = device).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "                else: \n",
    "                    next_state = None\n",
    "                \n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "     \n",
    "\n",
    "            #we only traina few times each epoch to avoid overfitting : \n",
    "            \n",
    "            for _ in range(train_steps): \n",
    "                critic_loss, actor_loss = optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn=torch.nn.MSELoss(size_average=True), actor_optimizer=actor_optimizer,\\\n",
    "                            critic_optimizer= critic_optimizer, device=device)\n",
    "                if actor_loss is not None and critic_loss is not None: \n",
    "                    actor_losses.append(actor_loss)\n",
    "                    critic_losses.append(critic_loss)\n",
    "    \n",
    "            target_critic_state_dict = target_critic.state_dict()\n",
    "            critic_state_dict = critic.state_dict()\n",
    "\n",
    "            target_actor_state_dict = target_actor.state_dict()\n",
    "            actor_state_dict = actor.state_dict()\n",
    "            \n",
    "            # we update our target networks with the weights of our primary networks (they really need to clear up terminology here lol)\n",
    "            \n",
    "            for key in actor_state_dict.keys():\n",
    "                #we perform soft updates as follows:\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_actor_state_dict[key] = actor_state_dict[key]*tau + target_actor_state_dict[key]*(1-tau)\n",
    "                target_actor.load_state_dict(target_actor_state_dict)\n",
    "            \n",
    "            for key in critic_state_dict.keys():    \n",
    "                target_critic_state_dict[key] = critic_state_dict[key]*tau + target_critic_state_dict[key]*(1-tau)\n",
    "                target_critic.load_state_dict(target_critic_state_dict)\n",
    "                \n",
    "            rewards_per_cycle.append(cum_reward)\n",
    "        \n",
    "\n",
    "\n",
    "        reward_per_episode.append(sum(rewards_per_cycle) / len(rewards_per_cycle))    \n",
    "        clear_output(wait=True) # Clear the output of the current cell receiving output\n",
    "        plt.plot(reward_per_episode)\n",
    "        plt.title('Average Cumulative Reward per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.show()\n",
    "        \n",
    "        if len(critic_losses) > 0: \n",
    "            actor_loss_per_episode.append(sum(actor_losses) / len(actor_losses))       \n",
    "            critic_loss_per_episode.append(sum(critic_losses) / len(critic_losses))       \n",
    "\n",
    "            plt.plot(actor_loss_per_episode)\n",
    "            plt.title('Actor Loss per Epoch')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Actor loss')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(critic_loss_per_episode)\n",
    "            plt.title('Critic Loss per Epoch')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Critic Loss')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams \n",
    "NUM_EPOCHS = 1000  \n",
    "GAMMA = 0.99 \n",
    "ALPHA_ACTOR = 1e-4 \n",
    "ALPHA_CRITIC = 3e-4\n",
    "BATCH_SIZE = 64 #we want a relatively large batch size to leverage GPU parallelism but at the same time we want this number to be far \n",
    "                #smaller than our memory capacity so that we can sample randomly at the same time\n",
    "NUM_CYCLES = 20 \n",
    "TRAIN_STEPS = 1500 \n",
    "TAU = 0.001\n",
    "NUM_ROLLOUTS = 30\n",
    "L2_REG = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BUFFER_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = ReplayMemory(BUFFER_SIZE)\n",
    "\n",
    "actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "actor.to(DEVICE)\n",
    "target_actor.to(DEVICE)\n",
    "\n",
    "critic.to(DEVICE)\n",
    "target_critic.to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch: 20000, training starts after 2.5 epochs, the replay buffer fills up in 50.0 epochs, we train based on 64000 samples per epoch\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = NUM_ROLLOUTS * NUM_CYCLES\n",
    "train_after_epochs = TRAIN_AFTER_STEPS / steps_per_epoch\n",
    "epochs_till_buffer_full = BUFFER_SIZE / steps_per_epoch\n",
    "train_samples_per_epoch = TRAIN_STEPS * BATCH_SIZE * NUM_CYCLES\n",
    "print(f\"steps per epoch: {steps_per_epoch}, training starts after {train_after_epochs} epochs, the replay buffer fills up in {epochs_till_buffer_full} epochs, we train based on {train_samples_per_epoch} samples\\\n",
    " per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=ALPHA_ACTOR, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=ALPHA_CRITIC, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i at epoch end: 1000\n"
     ]
    }
   ],
   "source": [
    "trainer(num_epochs=NUM_EPOCHS, num_cycles=NUM_CYCLES,num_rollouts=NUM_ROLLOUTS, train_steps=TRAIN_STEPS, batch_size=BATCH_SIZE, actor=actor, critic=critic, target_actor=target_actor, target_critic=target_critic,\\\n",
    "         gamma=GAMMA, device=DEVICE, replay_buffer=replay_buffer, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, tau=TAU, epochs_till_buffer_full= epochs_till_buffer_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetah-v4', render_mode = \"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_CYCLES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     action \u001b[39m=\u001b[39m actor(state)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 15\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     17\u001b[0m     test_epoch_end \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n\u001b[1;32m     19\u001b[0m     cum_reward\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mreward\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/half_cheetah_v4.py:209\u001b[0m, in \u001b[0;36mHalfCheetahEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    201\u001b[0m info \u001b[39m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mx_position\u001b[39m\u001b[39m\"\u001b[39m: x_position_after,\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mx_velocity\u001b[39m\u001b[39m\"\u001b[39m: x_velocity,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreward_run\u001b[39m\u001b[39m\"\u001b[39m: forward_reward,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreward_ctrl\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m-\u001b[39mctrl_cost,\n\u001b[1;32m    206\u001b[0m }\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m observation, reward, terminated, \u001b[39mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:404\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m data[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    403\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_viewer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_rendering.py:544\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    543\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 544\u001b[0m         update()\n\u001b[1;32m    545\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    547\u001b[0m \u001b[39m# clear markers\u001b[39;00m\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_rendering.py:522\u001b[0m, in \u001b[0;36mViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[39mfor\u001b[39;00m gridpos, [t1, t2] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_overlays\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    514\u001b[0m             mujoco\u001b[39m.\u001b[39mmjr_overlay(\n\u001b[1;32m    515\u001b[0m                 mujoco\u001b[39m.\u001b[39mmjtFontScale\u001b[39m.\u001b[39mmjFONTSCALE_150,\n\u001b[1;32m    516\u001b[0m                 gridpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcon,\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0m     glfw\u001b[39m.\u001b[39;49mswap_buffers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow)\n\u001b[1;32m    523\u001b[0m glfw\u001b[39m.\u001b[39mpoll_events()\n\u001b[1;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m (\n\u001b[1;32m    525\u001b[0m     time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m render_start\n\u001b[1;32m    526\u001b[0m )\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/glfw/__init__.py:2377\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2371\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \n\u001b[1;32m   2374\u001b[0m \u001b[39m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[39m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m     _glfw\u001b[39m.\u001b[39;49mglfwSwapBuffers(window)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/glfw/__init__.py:686\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    679\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    687\u001b[0m         \u001b[39mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    688\u001b[0m         \u001b[39mif\u001b[39;00m _exc_info_from_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_rewards = []\n",
    "for test in range(NUM_TEST_CYCLES): \n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device = DEVICE).unsqueeze(0)\n",
    "    test_epoch_end = False\n",
    "    cum_reward = 0 \n",
    "\n",
    "    while not test_epoch_end: \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            action = actor(state).squeeze().cpu().detach().numpy()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            test_epoch_end = terminated or truncated\n",
    "\n",
    "            cum_reward+=reward\n",
    "            if not terminated: \n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                        device = DEVICE).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "            else: \n",
    "                next_state = None\n",
    "\n",
    "            state = next_state\n",
    "    test_rewards.append(cum_reward)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.plot(test_rewards)\n",
    "    plt.title('Average Cumulative Reward per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts on results \n",
    "\n",
    "After training for a significant amount of time, although the cheetah was able to learn a policy that increased reward, it was a local maxima. The cheetah learned to flip to its back in the first few timesteps and then worm its way really fast forwards. While this is progress from previous training runs, it's not ideal. We can get an average test reward of around 1850, but we know that within this environment its possible for us to hit higher rewards up to the 10,000 range. \n",
    "\n",
    "My initial hunch is to reduce the critic's learning rate. I think the critic highly favoring such scenarios might be steering the actor in the direction of this policy, and that decreasing the critic's learning rate potentially by an order of magnitude might allow for more exploration taking our agent closer to an actual optimal policy. \n",
    "\n",
    "I'm not sure changing the exploration rate would do anything, considering a noise factor of 0.3 is already pretty high. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
