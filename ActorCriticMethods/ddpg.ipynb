{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "In this notebook we explore a neural network based implementation of the Actor-Critic method for Reinforcement Learning. This algorithm can be seen as the convergence of the breakthroughs that came from the DQN paper with the actor-critic algorithm. This algorithm, DDPG, applies the concepts of experience replay using a replay buffer to mitigate the sequential nature of the data being sampled by interaction with the environment, and instead learns by sampling a batch of random samples of experience saved in the replay buffer for gradient based optimization. \n",
    "\n",
    "In general, actor-critic algorithms comprise of two parts, an actor and a critic. Both of these parts are learned from data in the form of multilayer perceptrons rather than manual mappings or hand engineered features. The actor here is our policy network. Given an observation as input, the actor picks out the best action possible based on its weights. Since our actor directly picks out a singular action instead of a probability of actions, we say this follows the \"deterministic\" policy gradients algorithm, thus the \"deterministic\" in DDPG. The critic, on the other hand, is used to provide feedback to the actor. The critic network predicts the expected value of a state action pair (action picked out by the actor network), which is what is our actor's goal to maximize.\n",
    "\n",
    "We use the concept of a target network (as in DQN, refer to /reinforcement_learning/DQN for more) in order to stabilize the learning process. \n",
    "\n",
    "DDPG was designed to work well in continuous action spaces, so we run our experiences on the mujoco Half-Cheetah-v4 environment that provides us with a \"half\" cheetah as an agent whose goal is to travel as much in the forward direction as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.09100021, -0.07784483,  0.00201687,  0.05604557,  0.0026025 ,\n",
       "        -0.04051946, -0.01854231,  0.04961565,  0.05610586,  0.03195362,\n",
       "         0.11792976,  0.13226129,  0.13052194,  0.03976153,  0.14605028,\n",
       "         0.05408807,  0.02430947]),\n",
       " {})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v4')#, render_mode = \"human\") uncomment to render \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "As mentioned previously, the DDPG algorithm shines in continuous action spaces. Our cheetah has 6 pivots upon which it can execute forces from the -1.0 to 1.0 range, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (6,), float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_ACTIONS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (17,), float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 17 dimensional observation space. Our observations consist of positional values of the different pivots on the cheetah, their velocities and angular velocities with respect to our different euclidean coordinate frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_OBSERVATIONS = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a class for our replay buffer\n",
    "Nothing changes here v/s the DQN implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple function to sample some experience from memory randomly \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Critic (Q-Value function)\n",
    "The differences between this network and the Deep Q Network used for pure DQN are: \n",
    "- We're now evaluating a single action that has already been selected by the actor so the first linear layer accepts our observation concatenated with our action \n",
    "- Since we're evaluating only one action, we output the value estimate for that singluar action only instead of the set of possible actions giving our network an output dimension of 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_Q(nn.Module):\n",
    "\n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(critic_Q, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space + dim_actions, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        x = torch.cat([observation, action], dim=-1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network \n",
    "A multilayer perceptron that accepts our observation as input and outputs a deterministic action to be taken at timestep t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_Policy(nn.Module): \n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(actor_Policy, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #deterministically outputting a single action \n",
    "        self.layer3 = nn.Linear(128, dim_actions) #output has the specified dimensionality to specify actions with respect to \n",
    "                                                  #all action dimensions (in the case of our example, this is 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.tanh(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AFTER_STEPS = 5e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function Overview\n",
    "\n",
    "The following function represents the optimization steps. If our replay buffer is at capacity, we:\n",
    "\n",
    "- Sample a batch of transitions from memory. This initial step involves drawing a selection of varied transitions from our repository of stored experiences.\n",
    "\n",
    "- Compute the expected value using the critic network. \n",
    "\n",
    "- Run inference on target network to determine what our \"next action\" would be given our sampled \"next_state\"\n",
    "\n",
    "- Compute the target value by running inference on the target critic given the next state and next action from the above step \n",
    "\n",
    "- Given the predicted value, sampled reward and target next state/action value, compute the TD error\n",
    "\n",
    "- Compute the loss between the predicted and target values and then optimize the critic network. \n",
    "\n",
    "- Use the negative log of the mean value of the values outputted by the critic as the loss for the actor. We want to maximize this mean. \n",
    "\n",
    "Through this sequence of steps, the function continuously enhances the model's predictive performance along with control performance by learning from past errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, actor, critic, target_actor, target_critic, loss_fn, actor_optimizer, critic_optimizer, device):\n",
    "    \n",
    "    if len(memory) < 25e3: \n",
    "        return None, None\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions)) #batches our data as follows: ((state1, state2, ..), (action1, action2, ...), ...)\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    \n",
    "    predicted_values = critic(states, actions) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = target_actor(non_final_next_states)  \n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_critic(non_final_next_states, next_actions).squeeze(1)  \n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # if the state is non terminal, we add to it the expected value of the next state \n",
    "    td_error = target_values - predicted_values \n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(critic.parameters(), 100)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    actor_loss = -critic(states,actor(states)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    return loss, actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_epochs, num_cycles, num_rollouts, train_steps, batch_size, target_actor, actor, target_critic, critic, gamma, device, \\\n",
    "            replay_buffer, actor_optimizer, critic_optimizer, tau, epochs_till_buffer_full): \n",
    "    \n",
    "    reward_per_episode = []\n",
    "    noise_factor = 0.2 \n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "\n",
    "        rewards_per_cycle = []\n",
    "        \n",
    "        for cycle in range(num_cycles): \n",
    "            \n",
    "            cum_reward = 0\n",
    "            epoch_end = False\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "            while not epoch_end:\n",
    "\n",
    "            #for rollout in range(num_rollouts):    \n",
    "                action = actor(state).squeeze().cpu().detach().numpy()\n",
    "                \n",
    "                noise = np.random.normal(0, noise_factor, size=env.action_space.shape) # Adjust the scale (0.2) as needed\n",
    "                \n",
    "                action = np.clip(action + noise, env.action_space.low, env.action_space.high) #clip so that our actions never exceed the allowed thresholds \n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                epoch_end = terminated or truncated\n",
    "\n",
    "                cum_reward+=reward\n",
    "                \n",
    "                if not terminated: \n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                        device = device).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "                else: \n",
    "                    next_state = None\n",
    "                \n",
    "                action = torch.tensor(action, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward = torch.tensor(reward, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                \n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "\n",
    "            #we only traina few times each epoch to avoid overfitting : \n",
    "            if not cycle % (5): \n",
    "                for _ in range(train_steps): \n",
    "                    critic_loss, actor_loss = optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn=torch.nn.MSELoss(size_average=True), actor_optimizer=actor_optimizer,\\\n",
    "                                critic_optimizer= critic_optimizer, device=device)\n",
    "        \n",
    "            target_critic_state_dict = target_critic.state_dict()\n",
    "            critic_state_dict = critic.state_dict()\n",
    "\n",
    "            target_actor_state_dict = target_actor.state_dict()\n",
    "            actor_state_dict = actor.state_dict()\n",
    "            \n",
    "            # we update our target networks at the specified frequency to save computational cost while at the same time updating them \n",
    "            #frequently enough to stabilize the learning process \n",
    "            \n",
    "            \n",
    "            for key in actor_state_dict.keys():\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_actor_state_dict[key] = actor_state_dict[key]*tau + target_actor_state_dict[key]*(1-tau)\n",
    "                target_actor.load_state_dict(target_actor_state_dict)\n",
    "            \n",
    "            for key in critic_state_dict.keys():    \n",
    "                target_critic_state_dict[key] = critic_state_dict[key]*tau + target_critic_state_dict[key]*(1-tau)\n",
    "                target_critic.load_state_dict(target_critic_state_dict)\n",
    "            \n",
    "            rewards_per_cycle.append(cum_reward)\n",
    "        \n",
    "        reward_per_episode.append(sum(rewards_per_cycle) / len(rewards_per_cycle))        \n",
    "        \n",
    "        clear_output(wait=True) # Clear the output of the current cell receiving output\n",
    "        plt.plot(reward_per_episode)\n",
    "        plt.title('Average Cumulative Reward per Epoch')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams \n",
    "NUM_EPOCHS = 500  \n",
    "GAMMA = 0.99 \n",
    "ALPHA_ACTOR = 1e-4 \n",
    "ALPHA_CRITIC = 1e-3\n",
    "BATCH_SIZE = 64 #we want a relatively large batch size to leverage GPU parallelism but at the same time we want this number to be far \n",
    "                #smaller than our memory capacity so that we can sample randomly at the same time\n",
    "NUM_CYCLES = 20 \n",
    "TRAIN_STEPS = 50 \n",
    "TAU = 0.001\n",
    "NUM_ROLLOUTS = 1000\n",
    "L2_REG = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BUFFER_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = ReplayMemory(BUFFER_SIZE)\n",
    "\n",
    "actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "actor.to(DEVICE)\n",
    "target_actor.to(DEVICE)\n",
    "\n",
    "critic.to(DEVICE)\n",
    "target_critic.to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch: 20000, training starts after 2.5 epochs, the replay buffer fills up in 50.0 epochs, we train based on 64000 samples per epoch\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = NUM_ROLLOUTS * NUM_CYCLES\n",
    "train_after_epochs = TRAIN_AFTER_STEPS / steps_per_epoch\n",
    "epochs_till_buffer_full = BUFFER_SIZE / steps_per_epoch\n",
    "train_samples_per_epoch = TRAIN_STEPS * BATCH_SIZE * NUM_CYCLES\n",
    "print(f\"steps per epoch: {steps_per_epoch}, training starts after {train_after_epochs} epochs, the replay buffer fills up in {epochs_till_buffer_full} epochs, we train based on {train_samples_per_epoch} samples\\\n",
    " per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=ALPHA_ACTOR, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=ALPHA_CRITIC, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(num_epochs=NUM_EPOCHS, num_cycles=NUM_CYCLES,num_rollouts=NUM_ROLLOUTS, train_steps=TRAIN_STEPS, batch_size=BATCH_SIZE, actor=actor, critic=critic, target_actor=target_actor, target_critic=target_critic,\\\n",
    "         gamma=GAMMA, device=DEVICE, replay_buffer=replay_buffer, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, tau=TAU, epochs_till_buffer_full= epochs_till_buffer_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on first training run: \n",
    "\n",
    "For my first training run, the parameters I used were: \n",
    "\n",
    "NUM_EPOCHS = 500  \n",
    "GAMMA = 0.99 \n",
    "ALPHA_ACTOR = 3e-4 \n",
    "ALPHA_CRITIC = 3e-4\n",
    "BATCH_SIZE = 32 \n",
    "NUM_CYCLES = 20 \n",
    "TRAIN_STEPS = 20 \n",
    "TAU = 0.005\n",
    "NUM_ROLLOUTS = 1000\n",
    "L2_REG = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "I tweaked hyperparameters around a bit, but most combinations in this ballpark led to the agent being stuck in some local minima. This was either the agent doing a flip onto its back and then moving forward a little bit or sticking its head to the ground and crawling forward really slowly. When I implemented things this way, I set up a decaying noise factor which went from 0.5 to 0.1 in around 100 epochs. The idea was to explore early and aggressively and then learn from a diversity of different experiences. I think due to the largeness of the state and action spaces, what was happening is my agent found some local minima and then stuck to it, only changing its apparent policy every time the noise decayed. Instead of this, I changed my parameters to the current ones you see used above, following an online guide reluctantly due to how data hungry this new strategy is. With my measly A2000 laptop it might end up being 30-50 hours of training, but we're here for the ride. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
