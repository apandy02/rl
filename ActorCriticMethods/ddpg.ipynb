{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "In this notebook we explore a neural network based implementation of the Actor-Critic method for Reinforcement Learning. This algorithm can be seen as the convergence of the breakthroughs that came from the DQN paper with the actor-critic algorithm. This algorithm, DDPG, applies the concepts of experience replay using a replay buffer to mitigate the sequential nature of the data being sampled by interaction with the environment, and instead learns by sampling a batch of random samples of experience saved in the replay buffer for gradient based optimization. \n",
    "\n",
    "In general, actor-critic algorithms comprise of two parts, an actor and a critic. Both of these parts are learned from data in the form of multilayer perceptrons rather than manual mappings or hand engineered features. The actor here is our policy network. Given an observation as input, the actor picks out the best action possible based on its weights. Since our actor directly picks out a singular action instead of a probability of actions, we say this follows the \"deterministic\" policy gradients algorithm, thus the \"deterministic\" in DDPG. The critic, on the other hand, is used to provide feedback to the actor. The critic network predicts the expected value of a state action pair (action picked out by the actor network), which is what is our actor's goal to maximize.\n",
    "\n",
    "We use the concept of a target network (as in DQN, refer to /reinforcement_learning/DQN for more) in order to stabilize the learning process. \n",
    "\n",
    "DDPG was designed to work well in continuous action spaces, so we run our experiences on the mujoco Half-Cheetah-v4 environment that provides us with a \"half\" cheetah as an agent whose goal is to travel as much in the forward direction as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetah-v4', render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "As mentioned previously, the DDPG algorithm shines in continuous action spaces. Our cheetah has 6 pivots upon which it can execute forces from the -1.0 to 1.0 range, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (6,), float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_ACTIONS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (17,), float64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 17 dimensional observation space. Our observations consist of positional values of the different pivots on the cheetah, their velocities and angular velocities with respect to our different euclidean coordinate frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_OBSERVATIONS = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a class for our replay buffer\n",
    "Nothing changes here v/s the DQN implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tuple to store experience \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#Experience replay buffer object \n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        #contains a deque of specified buffer length \n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        #add new transition to buffer \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    #simple function to sample some experience from memory randomly \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Critic (Q-Value function)\n",
    "The differences between this network and the Deep Q Network used for pure DQN are: \n",
    "- We're now evaluating a single action that has already been selected by the actor so the first linear layer accepts our observation concatenated with our action \n",
    "- Since we're evaluating only one action, we output the value estimate for that singluar action only instead of the set of possible actions giving our network an output dimension of 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_Q(nn.Module):\n",
    "\n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(critic_Q, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space + dim_actions, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #linear downsizing to number of possible actions \n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        x = torch.cat([observation, action], dim=-1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network \n",
    "A multilayer perceptron that accepts our observation as input and outputs a deterministic action to be taken at timestep t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_Policy(nn.Module): \n",
    "    def __init__(self, len_observation_space, dim_actions):\n",
    "        super(actor_Policy, self).__init__()\n",
    "        #observations to 128 \n",
    "        self.layer1 = nn.Linear(len_observation_space, 128)\n",
    "        #linear transformation \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        #deterministically outputting a single action \n",
    "        self.layer3 = nn.Linear(128, dim_actions) #output has the specified dimensionality to specify actions with respect to \n",
    "                                                  #all action dimensions (in the case of our example, this is 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.tanh(self.layer3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function Overview\n",
    "\n",
    "The following function represents the optimization steps. If our replay buffer is at capacity, we:\n",
    "\n",
    "- Sample a batch of transitions from memory. This initial step involves drawing a selection of varied transitions from our repository of stored experiences.\n",
    "\n",
    "- Compute the expected value using the critic network. \n",
    "\n",
    "- Run inference on target network to determine what our \"next action\" would be given our sampled \"next_state\"\n",
    "\n",
    "- Compute the target value by running inference on the target critic given the next state and next action from the above step \n",
    "\n",
    "- Given the predicted value, sampled reward and target next state/action value, compute the TD error\n",
    "\n",
    "- Compute the loss between the predicted and target values and then optimize the critic network. \n",
    "\n",
    "- Use the negative log of the mean value of the values outputted by the critic as the loss for the actor. We want to maximize this mean. \n",
    "\n",
    "Through this sequence of steps, the function continuously enhances the model's predictive performance along with control performance by learning from past errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(gamma, batch_size, memory, actor, critic, target_actor, target_critic, loss_fn, actor_optimizer, critic_optimizer, device):\n",
    "    \n",
    "    if len(memory) < 10000: \n",
    "        return None, None\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions)) #batches our data as follows: ((state1, state2, ..), (action1, action2, ...), ...)\n",
    "    \n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    \n",
    "    predicted_values = critic(states, actions) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = target_actor(non_final_next_states)  \n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_critic(non_final_next_states, next_actions).squeeze(1)  \n",
    "    \n",
    "    target_values = (next_state_values * gamma) +rewards # if the state is non terminal, we add to it the expected value of the next state \n",
    "    td_error = target_values - predicted_values \n",
    "    \n",
    "    loss = loss_fn(predicted_values, target_values.unsqueeze(1))\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(critic.parameters(), 100)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    actor_loss = -critic(states,actor(states)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    return loss, actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_epochs, num_cycles, num_rollouts, train_steps, batch_size, target_actor, actor, target_critic, critic, gamma, device, replay_buffer, actor_optimizer, critic_optimizer, tau): \n",
    "    \n",
    "    reward_per_episode = []\n",
    "    noise_factor = 0.5 \n",
    "    epoch_losses_actor = []\n",
    "    epoch_losses_critic = []\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        losses_actor = []\n",
    "        losses_critic = []\n",
    "\n",
    "        #the following conditional decays our noise factor to reduce our rate of exploration as we progress through training \n",
    "        if (not epoch % 100) and (epoch > 0) and noise_factor > 0.1: \n",
    "            noise_factor-=0.05\n",
    "            print(noise_factor)\n",
    "        \n",
    "        for cycle in range(num_cycles): \n",
    "            \n",
    "            cum_reward = 0\n",
    "            epoch_end = False\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "            for __ in range(num_rollouts):\n",
    "                \n",
    "                action = actor(state).squeeze().cpu().detach().numpy()\n",
    "                noise = np.random.normal(0, noise_factor, size=env.action_space.shape) # Adjust the scale (0.2) as needed\n",
    "                \n",
    "                action = np.clip(action + noise, env.action_space.low, env.action_space.high)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                cum_reward+=reward\n",
    "                \n",
    "                if not terminated: \n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32, \\\n",
    "                        device = device).unsqueeze(0) #we only care about next_state if the state is non terminal\n",
    "                else: \n",
    "                    next_state = None\n",
    "                \n",
    "                action = torch.tensor(action, dtype=torch.int64, device=device).unsqueeze(0)\n",
    "                reward = torch.tensor(reward, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                replay_buffer.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "\n",
    "            if cycle == num_cycles-1: \n",
    "                for _ in range(train_steps): \n",
    "                    critic_loss, actor_loss = optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn=torch.nn.MSELoss(size_average=True), actor_optimizer=actor_optimizer,\\\n",
    "                                critic_optimizer= critic_optimizer, device=device)\n",
    "                    if critic_loss is not None and actor_loss is not None: \n",
    "                        losses_actor.append(actor_loss)\n",
    "                        losses_critic.append(critic_loss)\n",
    "            \n",
    "\n",
    "            #extract network parameters for both networks if cycle == num_cycles-1: \n",
    "                for _ in range(train_steps): \n",
    "                    critic_loss, actor_loss = optimize_model(gamma, batch_size, replay_buffer, actor, critic, target_actor, target_critic,loss_fn=torch.nn.MSELoss(size_average=True), actor_optimizer=actor_optimizer,\\\n",
    "                                critic_optimizer= critic_optimizer, device=device)\n",
    "                    '''if critic_loss is not None and actor_loss is not None: \n",
    "                        losses_actor.append(actor_loss)\n",
    "                        losses_critic.append(critic_loss)'''\n",
    "            target_critic_state_dict = target_critic.state_dict()\n",
    "            critic_state_dict = critic.state_dict()\n",
    "\n",
    "            target_actor_state_dict = target_actor.state_dict()\n",
    "            actor_state_dict = actor.state_dict()\n",
    "            \n",
    "            #update parameters with soft updates \n",
    "            for key in actor_state_dict.keys():\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_actor_state_dict[key] = actor_state_dict[key]*tau + target_actor_state_dict[key]*(1-tau)\n",
    "                target_actor.load_state_dict(target_actor_state_dict)\n",
    "            \n",
    "            for key in critic_state_dict.keys():    \n",
    "                target_critic_state_dict[key] = critic_state_dict[key]*tau + target_critic_state_dict[key]*(1-tau)\n",
    "                target_critic.load_state_dict(target_critic_state_dict)\n",
    "        \n",
    "        reward_per_episode.append(cum_reward)\n",
    "        \n",
    "        '''if len(losses_actor) > 0:     \n",
    "            epoch_losses_actor.append(sum(losses_actor) / len(losses_actor))\n",
    "            epoch_losses_critic.append(sum(losses_critic) / len(losses_critic))'''\n",
    "        \n",
    "        \n",
    "        clear_output(wait=True) # Clear the output of the current cell receiving output\n",
    "        plt.plot(reward_per_episode)\n",
    "        plt.title('Cumulative Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "replay_buffer = ReplayMemory(100000)\n",
    "\n",
    "actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_actor = actor_Policy(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "target_critic = critic_Q(len_observation_space=DIM_OBSERVATIONS, dim_actions=DIM_ACTIONS)\n",
    "\n",
    "actor.to(DEVICE)\n",
    "target_actor.to(DEVICE)\n",
    "\n",
    "critic.to(DEVICE)\n",
    "target_critic.to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams \n",
    "NUM_EPISODES = 2500 \n",
    "GAMMA = 0.99 \n",
    "ALPHA = 3e-4\n",
    "BATCH_SIZE = 64 #we want a relatively large batch size to leverage GPU parallelism but at the same time we want this number to be far \n",
    "                #smaller than our memory capacity so that we can sample randomly at the same time\n",
    "NUM_CYCLES = 5 \n",
    "TRAIN_STEPS = 6 \n",
    "TAU = 0.005\n",
    "NUM_ROLLOUTS = 100\n",
    "L2_REG = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=ALPHA, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=ALPHA, weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer(num_epochs\u001b[39m=\u001b[39;49mNUM_EPISODES, num_cycles\u001b[39m=\u001b[39;49mNUM_CYCLES,num_rollouts\u001b[39m=\u001b[39;49mNUM_ROLLOUTS, train_steps\u001b[39m=\u001b[39;49mTRAIN_STEPS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, actor\u001b[39m=\u001b[39;49mactor, critic\u001b[39m=\u001b[39;49mcritic, target_actor\u001b[39m=\u001b[39;49mtarget_actor, target_critic\u001b[39m=\u001b[39;49mtarget_critic,\\\n\u001b[1;32m      2\u001b[0m          gamma\u001b[39m=\u001b[39;49mGAMMA, device\u001b[39m=\u001b[39;49mDEVICE, replay_buffer\u001b[39m=\u001b[39;49mreplay_buffer, actor_optimizer\u001b[39m=\u001b[39;49mactor_optimizer, critic_optimizer\u001b[39m=\u001b[39;49mcritic_optimizer, tau\u001b[39m=\u001b[39;49mTAU)\n",
      "Cell \u001b[0;32mIn[91], line 29\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(num_epochs, num_cycles, num_rollouts, train_steps, batch_size, target_actor, actor, target_critic, critic, gamma, device, replay_buffer, actor_optimizer, critic_optimizer, tau)\u001b[0m\n\u001b[1;32m     26\u001b[0m noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, noise_factor, size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape) \u001b[39m# Adjust the scale (0.2) as needed\u001b[39;00m\n\u001b[1;32m     28\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(action \u001b[39m+\u001b[39m noise, env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m---> 29\u001b[0m next_state, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     31\u001b[0m cum_reward\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mreward\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated: \n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/half_cheetah_v4.py:209\u001b[0m, in \u001b[0;36mHalfCheetahEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    201\u001b[0m info \u001b[39m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mx_position\u001b[39m\u001b[39m\"\u001b[39m: x_position_after,\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mx_velocity\u001b[39m\u001b[39m\"\u001b[39m: x_velocity,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreward_run\u001b[39m\u001b[39m\"\u001b[39m: forward_reward,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreward_ctrl\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m-\u001b[39mctrl_cost,\n\u001b[1;32m    206\u001b[0m }\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m observation, reward, terminated, \u001b[39mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:404\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m data[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    403\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_viewer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_rendering.py:544\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    543\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 544\u001b[0m         update()\n\u001b[1;32m    545\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    547\u001b[0m \u001b[39m# clear markers\u001b[39;00m\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_rendering.py:522\u001b[0m, in \u001b[0;36mViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[39mfor\u001b[39;00m gridpos, [t1, t2] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_overlays\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    514\u001b[0m             mujoco\u001b[39m.\u001b[39mmjr_overlay(\n\u001b[1;32m    515\u001b[0m                 mujoco\u001b[39m.\u001b[39mmjtFontScale\u001b[39m.\u001b[39mmjFONTSCALE_150,\n\u001b[1;32m    516\u001b[0m                 gridpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcon,\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0m     glfw\u001b[39m.\u001b[39;49mswap_buffers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow)\n\u001b[1;32m    523\u001b[0m glfw\u001b[39m.\u001b[39mpoll_events()\n\u001b[1;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m (\n\u001b[1;32m    525\u001b[0m     time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m render_start\n\u001b[1;32m    526\u001b[0m )\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/glfw/__init__.py:2377\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2371\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \n\u001b[1;32m   2374\u001b[0m \u001b[39m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[39m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m     _glfw\u001b[39m.\u001b[39;49mglfwSwapBuffers(window)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/glfw/__init__.py:686\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    679\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    687\u001b[0m         \u001b[39mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    688\u001b[0m         \u001b[39mif\u001b[39;00m _exc_info_from_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(num_epochs=NUM_EPISODES, num_cycles=NUM_CYCLES,num_rollouts=NUM_ROLLOUTS, train_steps=TRAIN_STEPS, batch_size=BATCH_SIZE, actor=actor, critic=critic, target_actor=target_actor, target_critic=target_critic,\\\n",
    "         gamma=GAMMA, device=DEVICE, replay_buffer=replay_buffer, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, tau=TAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on first training run: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
