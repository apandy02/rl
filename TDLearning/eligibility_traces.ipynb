{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (Lambda) [In-Progress]\n",
    "We take the Temporal Difference Algorithm SARSA (state, action, reward, state', action') and introduce eligibility traces to it. One of the pitfalls of Temporal Difference learning methods for learning is that since they back up step by step (online), a reward is only associated to the last step that led to it. In Monte Carlo methods, we backup the whole sequence of actions, which has its own pitfalls (explored in /MonteCarloMethods/vanilla_monte_carlo.ipynb). Eligibility traces provide some sense of memory to our algorithm. We use this method to update state-action pairs based on their \"relevance\" to the reward. For example, if a state action pair occurs right before the terminal state, it might be considered more relevant than the first pair in the sequence. However, since the first state-action pair was part of our sequence, we assign some non-zero relevance to this pair that decays over time. This allows us to learn in a more general manner. \n",
    "\n",
    "Another method to bridge the gap between the pros and cons of Monte Carlo Methods and Temporal Difference methods is to backup based on some fixed n-step return. In this case, we have a fixed time horizon, which may not be the most suitable if we have an environment in which our episode lengths may differ drastically. Additionally, as n approaches the length of the episode, we effectively have Monte Carlo Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode = 'human')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "#print(\"We have {} action space and {} state space\".format(action_space, state_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns an array of action probabilities for a given state (a polic) \n",
    "#this policy is designed to be epsilon-greedy in relation to the state action value function Q \n",
    "def policy_fn(Q, num_actions, e, state):\n",
    "    action_probabilities = np.ones(num_actions) * (e/num_actions)\n",
    "    highest_action_value = np.argmax(Q[state])\n",
    "    action_probabilities[highest_action_value] += (1 - e)\n",
    "    \n",
    "    return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializations \n",
    "Q = np.zeros((state_space.n, action_space.n))\n",
    "returns = [[[]]*action_space.n for i in range(state_space.n)]\n",
    "pi = np.zeros((state_space.n, action_space.n))\n",
    "epochs_per_episode = []\n",
    "action_prev = 0 \n",
    "state_prev = 0 \n",
    "\n",
    "#hyperparams \n",
    "num_episodes = 500\n",
    "epsilon = 0.2\n",
    "gamma = 1\n",
    "alpha = 0.1\n",
    "lamda = 0.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Control with SARSA (lambda) using Dutch Eligibility Traces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 0\n",
      "Episode: 0 Epoch: 661\n",
      "Episode: 1 Epoch: 0\n",
      "Episode: 1 Epoch: 1283\n",
      "Episode: 2 Epoch: 0\n",
      "Episode: 2 Epoch: 1940\n",
      "Episode: 3 Epoch: 0\n",
      "Episode: 3 Epoch: 931\n",
      "Episode: 4 Epoch: 0\n",
      "Episode: 4 Epoch: 343\n",
      "Episode: 5 Epoch: 0\n",
      "Episode: 5 Epoch: 595\n",
      "Episode: 6 Epoch: 0\n",
      "Episode: 6 Epoch: 2252\n",
      "Episode: 7 Epoch: 0\n",
      "Episode: 7 Epoch: 884\n",
      "Episode: 8 Epoch: 0\n",
      "Episode: 8 Epoch: 2965\n",
      "Episode: 9 Epoch: 0\n",
      "Episode: 9 Epoch: 1019\n",
      "Episode: 10 Epoch: 0\n",
      "Episode: 10 Epoch: 4331\n",
      "Episode: 11 Epoch: 0\n",
      "Episode: 11 Epoch: 1647\n",
      "Episode: 12 Epoch: 0\n",
      "Episode: 12 Epoch: 676\n",
      "Episode: 13 Epoch: 0\n",
      "Episode: 13 Epoch: 3649\n",
      "Episode: 14 Epoch: 0\n",
      "Episode: 14 Epoch: 1628\n",
      "Episode: 15 Epoch: 0\n",
      "Episode: 15 Epoch: 890\n",
      "Episode: 16 Epoch: 0\n",
      "Episode: 16 Epoch: 143\n",
      "Episode: 17 Epoch: 0\n",
      "Episode: 17 Epoch: 852\n",
      "Episode: 18 Epoch: 0\n",
      "Episode: 18 Epoch: 1387\n",
      "Episode: 19 Epoch: 0\n",
      "Episode: 19 Epoch: 3487\n",
      "Episode: 20 Epoch: 0\n",
      "Episode: 20 Epoch: 112\n",
      "Episode: 21 Epoch: 0\n",
      "Episode: 21 Epoch: 220\n",
      "Episode: 22 Epoch: 0\n",
      "Episode: 22 Epoch: 2812\n",
      "Episode: 23 Epoch: 0\n",
      "Episode: 23 Epoch: 736\n",
      "Episode: 24 Epoch: 0\n",
      "Episode: 24 Epoch: 201\n",
      "Episode: 25 Epoch: 0\n",
      "Episode: 25 Epoch: 98\n",
      "Episode: 26 Epoch: 0\n",
      "Episode: 26 Epoch: 2210\n",
      "Episode: 27 Epoch: 0\n",
      "Episode: 27 Epoch: 1274\n",
      "Episode: 28 Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "E = np.zeros((state_space.n, action_space.n))\n",
    "time_start = datetime.now().time()\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    cumulative_reward = 0 \n",
    "    epoch = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    if type(state)==tuple: \n",
    "        state = state[0]\n",
    "    pi[state] = policy_fn(Q, action_space.n, epsilon, state)\n",
    "    action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "    \n",
    "    terminated = False \n",
    "    print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "    \n",
    "    while not terminated: \n",
    "\n",
    "        epoch+=1\n",
    "        if type(state)==tuple: \n",
    "            state = state[0]\n",
    "        \n",
    "        next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "        \n",
    "        pi[state] = policy_fn(Q, action_space.n, epsilon, state)\n",
    "        next_action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "        \n",
    "        if type(next_state)==tuple: \n",
    "            next_state = next_state[0]\n",
    "        \n",
    "        #calculate the temporal difference error for the predicted value of the state-action \n",
    "        #pair \n",
    "        td_error = reward + (gamma * Q[next_state][next_action]) - Q[state][action]\n",
    "        \n",
    "        # we use accumulating eligibility traces \n",
    "        E[state][action] = (E[state][action]) + 1\n",
    "        \n",
    "        #Backup state-action pair based on its TD error and eligibility trace\n",
    "        Q[state][action] = Q[state][action] + (alpha * td_error * E[state][action])\n",
    "        \n",
    "        #Decay all Eligibility traces\n",
    "        E = lamda * gamma * E\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        \n",
    "        if terminated: \n",
    "            print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "            epochs_per_episode.append(epoch)\n",
    "\n",
    "time_end = datetime.now().time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datetime1 \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mcombine(datetime\u001b[39m.\u001b[39mtoday(), time_start)\n\u001b[1;32m      2\u001b[0m datetime2 \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mcombine(datetime\u001b[39m.\u001b[39mtoday(), time_end)\n\u001b[1;32m      4\u001b[0m \u001b[39m# calculate the difference between the datetime objects\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "datetime1 = datetime.combine(datetime.today(), time_start)\n",
    "datetime2 = datetime.combine(datetime.today(), time_end)\n",
    "\n",
    "# calculate the difference between the datetime objects\n",
    "time_diff = (datetime2 - datetime1).total_seconds()\n",
    "print(\"Total time taken for 500 episodes: {}\".format(time_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "num_episode = list(range(1, len(epochs_per_episode) + 1))\n",
    "plt.bar(num_episode, epochs_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the learning process, it may seem that sarsa lambda is not the ideal choice in this scenario. The rate of convergence is far inferior to that of the off policy Q-learning method evaluated previously. \n",
    "\n",
    "On the other hand, this method seems to be outperforming the regular SARSA algorithm on this task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
