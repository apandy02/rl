{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration \n",
    "We write two algorithms as functions to be used with generic MDPs. The first of these is Policy iteration, and the second is Value iteration. Both of these methods are instances of Generalized Policy Iteration algorithms, where we evaluate a policy, then make the policy greedy with respect to the value function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration: \n",
    "Here, we perform policy evaluation and improvement one after the other till we get a policy that is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(states, action_space, reward_per_state, gamma, transition_probabilities): \n",
    "    theta = 1e-4\n",
    "    V = [0 for i in range(len(states))] #initialize Value function\n",
    "    pi = [None for i in range(len(states))] \n",
    "    max_iterations = 1e5\n",
    "    for i in range(max_iterations): \n",
    "        optimal_policy = True \n",
    "        for j in range(max_iterations): \n",
    "            max_diff = 0 \n",
    "            V_new = [0 for i in range(len(states))]\n",
    "            for state in states: \n",
    "                val = reward_per_state[state] \n",
    "                #calculate discounted return \n",
    "                for next_state in states: \n",
    "                    val+=reward_per_state[next_state]* transition_probabilities[state][next_state][pi[state]] * (gamma * V[next_state])\n",
    "                max_diff = max(max_diff, abs(V[s]-val))\n",
    "                V[s] = val\n",
    "            if max_diff < theta: \n",
    "                break \n",
    "        \n",
    "        for state in states: \n",
    "            current_value = V[s]\n",
    "            for action in action_space: \n",
    "                val = reward_per_state[state]\n",
    "                for next_state in states: \n",
    "                    val+=reward_per_state[next_state]* transition_probabilities[state][next_state][action] * (gamma * V[next_state])\n",
    "                if val > current_value: \n",
    "                    pi[state] = action \n",
    "                    V[state] = val \n",
    "                    optimal_policy = False\n",
    "        if optimal_policy: \n",
    "            break \n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "This method is more efficient (in most cases) when compared to the prior because we perform a singular backup during the evaluation phase. Here, we simply pick the action that maximizes our value function with one backup step. This can be computationally cheaper to perform than the algorithm above. \n",
    "Please Note, in this case we only build at deterministic policies. More often than not, we're going to want to build stochastic policies instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(action_space, states, reward_per_state, gamma, transition_probabilities): \n",
    "    max_iterations = 1e5 \n",
    "    theta = 1e-4\n",
    "    V = [0 for i in range(len(states))] #initialize Value function\n",
    "    pi = [None for i in range(len(states))] #Initialize policy \n",
    "    \n",
    "    #run until change in value is smaller than theta or max iterations counted \n",
    "    for i in range(max_iterations): \n",
    "        max_diff = 0 \n",
    "        V_new = [0 for i in range(len(states))]\n",
    "        for state in states: \n",
    "            max_val = 0 \n",
    "            for action in action_space: \n",
    "                val = reward_per_state[state] \n",
    "                #calculate discounted return \n",
    "                for next_state in states: \n",
    "                    val+=reward_per_state[next_state]* transition_probabilities[state][next_state][action] * (gamma * V[next_state])\n",
    "                if val > max_val: \n",
    "                    max_val = val\n",
    "                #pick action that maximizes value \n",
    "                if V[state] < val: \n",
    "                    pi[state] = action_space[action]\n",
    "            max_diff = max(max_diff, abs(V[state]-V_new[state]))\n",
    "        V = V_new \n",
    "        if max_diff < theta: \n",
    "            break \n",
    "    \n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating both GPI methods over different MDPs \n",
    "Over the next section we'll build some basic Markov Decision Processes and apply these two iterative methods to them. We can then compare runtime and convergence of the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
