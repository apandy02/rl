{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods for prediction and control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: OpenAI Taxi-v3\n",
    "\n",
    "For the purposes of this assignment, we use Taxi-v3 from OpenAI's Gym repository. This environment, its states, actions and goals are detailed on the following web page: https://www.gymlibrary.dev/environments/toy_text/taxi/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have Discrete(6) action space and Discrete(500) state space\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "print(\"We have {} action space and {} state space\".format(action_space, state_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-policy Monte Carlo Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns an array of action probabilities for a given state (a polic) \n",
    "#this policy is designed to be epsilon-greedy in relation to the state action value function Q \n",
    "def policy_fn(Q, num_actions, e, state):\n",
    "    action_probabilities = np.ones(num_actions) * (e/num_actions)\n",
    "    highest_action_value = np.argmax(Q[state])\n",
    "    action_probabilities[highest_action_value] += 1 - e\n",
    "    \n",
    "    return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "e = 0.5\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 2\n",
      "Episode: 0 Epoch: 3\n",
      "Episode: 0 Epoch: 4\n",
      "Episode: 0 Epoch: 5\n",
      "Episode: 0 Epoch: 6\n",
      "Episode: 0 Epoch: 7\n",
      "Episode: 0 Epoch: 8\n",
      "Episode: 0 Epoch: 9\n",
      "Episode: 0 Epoch: 10\n",
      "Episode: 0 Epoch: 11\n"
     ]
    }
   ],
   "source": [
    "Q = np.ones((state_space.n, action_space.n))\n",
    "returns = [[[]]*action_space.n for i in range(state_space.n)]\n",
    "pi = np.zeros((state_space.n, action_space.n))\n",
    "\n",
    "epochs_per_episode = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0 \n",
    "    epoch = 0\n",
    "    terminated = False \n",
    "    while not terminated: \n",
    "        \n",
    "        epoch+=1\n",
    "        print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "        if type(state)==tuple: \n",
    "            state = state[0]\n",
    "        \n",
    "        pi[state] = policy_fn(Q, action_space.n, e, state)\n",
    "        action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "        next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        #print(episode[(state, action)])\n",
    "        if terminated: \n",
    "            epochs_per_episode.append(epoch)\n",
    "            break \n",
    "        state = next_state\n",
    "        \n",
    "    if i % 10: \n",
    "        e-=0.1    \n",
    "    visited = []\n",
    "    return_till_now = 0 \n",
    "    returns = {}\n",
    "    states_in_episode = []\n",
    "    for k, (state, action, reward) in enumerate(episode): \n",
    "        if state not in states_in_episode: \n",
    "            states_in_episode.append(state)\n",
    "        if (state, action) in visited: \n",
    "            continue\n",
    "        else: \n",
    "            visited.append((state, action))\n",
    "            G = sum([r*(gamma**j) for j, (s, a, r) in enumerate(episode)])\n",
    "            if (state, action) in returns.keys(): \n",
    "                returns[(state, action)].append(G)\n",
    "            else: \n",
    "                returns[(state, action)] = [G]\n",
    "            Q[state][action] = sum(returns[(state, action)])/len(returns[(state, action)])\n",
    "            \n",
    "    for state in states_in_episode: \n",
    "        pi[state] = policy_fn(Q, action_space.n, e, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.15, 0.15, 0.25, 0.15, 0.15, 0.15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       ...,\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [ 1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,  1.000e+00,\n",
       "         1.000e+00],\n",
       "       [-2.324e+03, -2.324e+03,  1.000e+00, -2.324e+03,  1.000e+00,\n",
       "        -2.324e+03]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0, reward = -10 and termination: False\n",
      "At step 1, reward = -1 and termination: False\n",
      "At step 2, reward = -1 and termination: False\n",
      "At step 3, reward = -1 and termination: False\n",
      "At step 4, reward = -1 and termination: False\n",
      "At step 5, reward = -1 and termination: False\n",
      "At step 6, reward = -1 and termination: False\n",
      "At step 7, reward = -1 and termination: False\n",
      "At step 8, reward = -1 and termination: False\n",
      "At step 9, reward = -1 and termination: False\n",
      "At step 10, reward = -1 and termination: False\n",
      "At step 11, reward = -1 and termination: False\n",
      "At step 12, reward = -1 and termination: False\n",
      "At step 13, reward = -1 and termination: False\n",
      "At step 14, reward = -1 and termination: False\n",
      "At step 15, reward = -1 and termination: False\n",
      "At step 16, reward = -1 and termination: False\n",
      "At step 17, reward = -1 and termination: False\n",
      "At step 18, reward = -1 and termination: False\n",
      "At step 19, reward = -1 and termination: False\n",
      "At step 20, reward = -1 and termination: False\n",
      "At step 21, reward = -1 and termination: False\n",
      "At step 22, reward = -1 and termination: False\n",
      "At step 23, reward = -1 and termination: False\n",
      "At step 24, reward = -1 and termination: False\n",
      "At step 25, reward = -1 and termination: False\n",
      "At step 26, reward = -1 and termination: False\n",
      "At step 27, reward = -1 and termination: False\n",
      "At step 28, reward = -1 and termination: False\n",
      "At step 29, reward = -1 and termination: False\n",
      "At step 30, reward = -1 and termination: False\n",
      "At step 31, reward = -1 and termination: False\n",
      "At step 32, reward = -1 and termination: False\n",
      "At step 33, reward = -1 and termination: False\n",
      "At step 34, reward = -1 and termination: False\n",
      "At step 35, reward = -1 and termination: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(state)\u001b[39m==\u001b[39m\u001b[39mtuple\u001b[39m: \n\u001b[1;32m      6\u001b[0m     state \u001b[39m=\u001b[39m state[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m      9\u001b[0m \u001b[39m#time.sleep(0.03)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pi[state] \u001b[39m=\u001b[39m policy_fn(Q, action_space\u001b[39m.\u001b[39mn, e, state)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py:416\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    415\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    417\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    419\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    420\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_start = env.reset()\n",
    "i = 0\n",
    "e = 0.03 #make policy less exploratory while testing \n",
    "for i in range(1000): \n",
    "    if type(state)==tuple: \n",
    "        state = state[0]\n",
    "    \n",
    "    env.render()\n",
    "    #time.sleep(0.03)\n",
    "    pi[state] = policy_fn(Q, action_space.n, e, state)\n",
    "    action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "    next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "    print(\"At step {}, reward = {} and termination: {}\".format(i, reward, terminated))\n",
    "    \n",
    "    #print(episode[(state, action)])\n",
    "    if terminated: \n",
    "        break \n",
    "    state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Results \n",
    "Due to the extremely stochastic nature of the average based monte carlo algorithm during the first few episodes, runtime per episode was too high to keep running. However, this led to some learnings: As the size of the state and action spaces grows, the number of epochs required to reach a terminal state grows rapidly. Since this method learns offline (ie. not updating its values during interaction) the learning through the first few episodes is extremely slow. In the next notebook I will implement a temporal difference learning model that learns online and compare these two results. "
=======
    "### Notes \n",
    "The findings here seem pretty simple- Vanilla average-based monte carlo control is not good enough... the number of states and actions make it hard for the agent to experience good behavior and reach a terminal state in a reasonable number of iterations. Temporal Difference learning should be slightly better at learning from incomplete episodes and will be the basis for the next exercise "
>>>>>>> ca2f8264711332cfe83eade9482c1474385a64f9
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
