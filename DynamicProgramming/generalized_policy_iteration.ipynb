{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration \n",
    "We write two algorithms as functions to be used with generic MDPs. The first of these is Policy iteration, and the second is Value iteration. Both of these methods are instances of Generalized Policy Iteration algorithms, where we evaluate a policy, then make the policy greedy with respect to the value function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration: \n",
    "Here, we perform policy evaluation and improvement one after the other till we get a policy that is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(states, action_space, reward_per_state, gamma, transition_probabilities): \n",
    "    theta = 1e-4\n",
    "    V = [0 for i in range(len(states))] #initialize Value function\n",
    "    pi = [0 for i in range(len(states))]\n",
    "    max_iterations = 10000\n",
    "    for i in range(max_iterations): \n",
    "        \n",
    "        #perform policy evaluation \n",
    "        for j in range(max_iterations): \n",
    "            max_diff = 0 \n",
    "            \n",
    "            for state in states: \n",
    "                val = 0\n",
    "                for next_state in states: \n",
    "                    prob = transition_probabilities[state][next_state][pi[state]]\n",
    "                    next_state_reward = reward_per_state[next_state]\n",
    "                    val += prob * (gamma * V[next_state] + next_state_reward)\n",
    "                \n",
    "                max_diff = max(max_diff, abs(V[state]-val))\n",
    "                V[state] = val\n",
    "            \n",
    "            if max_diff < theta: \n",
    "                break \n",
    "        #perform policy improvement \n",
    "        optimal_policy = True \n",
    "        for state in states: \n",
    "            current_value = V[state]\n",
    "            for action in action_space: \n",
    "                val = 0\n",
    "                for next_state in states: \n",
    "                    prob = transition_probabilities[state][next_state][action]\n",
    "                    val += prob * (gamma * V[next_state] + reward_per_state[next_state])\n",
    "                \n",
    "                if val > current_value: \n",
    "                    pi[state] = action \n",
    "                    optimal_policy = False\n",
    "        \n",
    "        if optimal_policy: \n",
    "            break \n",
    "\n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "This method is more efficient (in most cases) when compared to the prior because we perform a singular backup during the evaluation phase. Here, we simply pick the action that maximizes our value function with one backup step. This can be computationally cheaper to perform than the algorithm above. \n",
    "Please Note, in this case we only build at deterministic policies. More often than not, we're going to want to build stochastic policies instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(action_space, states, reward_per_state, gamma, transition_probabilities): \n",
    "    max_iterations = 10000 \n",
    "    theta = 1e-4\n",
    "    V = [0 for i in range(len(states))] #initialize Value function\n",
    "    pi = [None for i in range(len(states))] #Initialize policy \n",
    "    \n",
    "    #run until change in value is smaller than theta or max iterations counted \n",
    "    for i in range(max_iterations): \n",
    "        max_diff = 0 \n",
    "        for state in states: \n",
    "            old_val = V[state]\n",
    "            max_val = 0 \n",
    "            for action in action_space: \n",
    "                val = reward_per_state[state] \n",
    "                #calculate discounted return \n",
    "                for next_state in states: \n",
    "                    prob = transition_probabilities[state][next_state][action]\n",
    "                    val += prob * (gamma * V[next_state] + reward_per_state[next_state])\n",
    "                if val > max_val:   \n",
    "                    max_val = val\n",
    "                #pick action that maximizes value \n",
    "                if V[state] < val: \n",
    "                    pi[state] = action_space[action]\n",
    "                    V[state] = val\n",
    "            \n",
    "            max_diff = max(max_diff, abs(V[state]-old_val))\n",
    "\n",
    "        if max_diff < theta: \n",
    "            break \n",
    "    \n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating both GPI methods over different MDPs \n",
    "Over the next section we'll build some basic Markov Decision Processes and apply these two iterative methods to them. We can then compare runtime and convergence of the two. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A boxing MDP. States are as follows: wide stanced (0), narrow stanced (1), reverse stanced (2)\n",
    "Actions are: widen stance, narrow stance, switch stance, throw punch \n",
    "when we widen our stance, we end up in wide stance (0), when we narrow we end up in narrow and when we reverse we end up in 2. When we throw a punch we can end up in a wide stance or a narrow stance with 50% probability. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The above defined MDP does not make any real life sense, but was a quick experiment to come up with an MDP formulation to play around with. during the process of learning, I tweaked all of the numbers in the rewards and transition probability matrices below to see how the optimal policy selected evolves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [0, 1, 2]\n",
    "actions = [0, 1, 2, 3]\n",
    "\n",
    "rewards = [5, -5, -9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11.42855177369, 5.714275886845, 1.7142758868449999], [0, 0, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transition_probs = [[[0.6, 0, 0, 0.4], [0, .8, 0.1,0.1], [0, 0, 1, 0]], \n",
    "                    [[1, 0, 0, 0], [0, 0.7, 0, 0.3], [0, 0, 0.9, 0.1]], \n",
    "                    [[1, 0, 0, 0], [0,0,1,0], [0, 0, 0,1]]]\n",
    "\n",
    "gamma = 0.5 \n",
    "\n",
    "value_iteration(action_space=actions, states=states, reward_per_state=rewards, gamma=gamma, transition_probabilities=transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11.42855177369, 5.714275886845, 1.7142758868449999], [0, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = [0, 1, 2]\n",
    "actions = [0, 1, 2, 3]\n",
    "\n",
    "rewards = [5, -5, -9]\n",
    "\n",
    "transition_probs = [[[0.6, 0, 0, 0.4], [0, .8, 0.1,0.1], [0, 0, 1, 0]], \n",
    "                    [[1, 0, 0, 0], [0, 0.7, 0, 0.3], [0, 0, 0.9, 0.1]], \n",
    "                    [[1, 0, 0, 0], [0,0,1,0], [0, 0, 0,1]]]\n",
    "\n",
    "gamma = 0.5 \n",
    "\n",
    "value_iteration(action_space=actions, states=states, reward_per_state=rewards, gamma=gamma, transition_probabilities=transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6, 0, 0, 0.4], [0, 0.8, 0.1, 0.1], [0, 0, 1, 0]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.285714285714286, 7.142857142857142, 7.142857142857142], [0, 0, 0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(states, actions, rewards, gamma, transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
