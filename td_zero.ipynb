{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: OpenAI Taxi-v3\n",
    "\n",
    "For the purposes of this assignment (as in the previous notebook), we use Taxi-v3 from OpenAI's Gym repository. This environment, its states, actions and goals are detailed on the following web page: https://www.gymlibrary.dev/environments/toy_text/taxi/. The environment is kept the same as the vanilla monte-carlo algorithm to be able to compare efficiency and speed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have Discrete(6) action space and Discrete(500) state space\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "print(\"We have {} action space and {} state space\".format(action_space, state_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-policy Monte Carlo Control "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing state and action previous as zero for the SARSA update step, we could sample the next action during training and calculate the difference, but instead we choose to do it this way so that the actions used in the update step are those that have been selected in the episode. If there's something wrong with this method don't be afraid to let me know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns an array of action probabilities for a given state (a polic) \n",
    "#this policy is designed to be epsilon-greedy in relation to the state action value function Q \n",
    "def policy_fn(Q, num_actions, e, state):\n",
    "    action_probabilities = np.ones(num_actions) * (e/num_actions)\n",
    "    highest_action_value = np.argmax(Q[state])\n",
    "    action_probabilities[highest_action_value] += (1 - e)\n",
    "    \n",
    "    return action_probabilities\n",
    "\n",
    "def update_params(Q, state, action, reward, s_next, alpha, gamma) -> None: \n",
    "    old_val = Q[state, action]\n",
    "    a_next = np.max(Q[s_next])\n",
    "    new_val = old_val + alpha * (reward + gamma * a_next - old_val)\n",
    "    Q[state, action] = new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializations \n",
    "Q = np.zeros((state_space.n, action_space.n))\n",
    "returns = [[[]]*action_space.n for i in range(state_space.n)]\n",
    "pi = np.zeros((state_space.n, action_space.n))\n",
    "state_prev = 0 # dummy initialization of previous state and action for step 0 \n",
    "action_prev = 0 \n",
    "epochs_per_episode = []\n",
    "\n",
    "#hyperparams \n",
    "num_episodes = 1000\n",
    "e = 0.1\n",
    "gamma = 0.6\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0 \n",
    "    epoch = 0\n",
    "    terminated = False \n",
    "    print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "    \n",
    "    while not terminated: \n",
    "        \n",
    "        epoch+=1\n",
    "        if type(state)==tuple: \n",
    "            state = state[0]\n",
    "        \n",
    "        pi[state] = policy_fn(Q, action_space.n, e, state)\n",
    "        action = np.random.choice(np.arange(action_space.n), p = pi[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, step_dict = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        update_params(Q, state, action, reward, next_state, alpha, gamma)\n",
    "        \n",
    "        if terminated: \n",
    "            print(\"Episode: {} Epoch: {}\".format(i, epoch))\n",
    "            epochs_per_episode.append(epoch)\n",
    "            break \n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learned policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
